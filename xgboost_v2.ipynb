{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_survival_probability(df, time_col='efs_time', event_col='efs'):\n",
    "    kmf = KaplanMeierFitter()\n",
    "    df_ = df.loc[(df[time_col] < 24) | (df[event_col] == 0)]\n",
    "    kmf.fit(df_[time_col], df_[event_col])\n",
    "    y = kmf.survival_function_at_times(df[time_col]).values\n",
    "    plt.hist(df.efs_time[df[event_col] == 0], bins=150, alpha=0.5, label=\"Event\")\n",
    "    plt.show()\n",
    "    #y = y / (y.max() - y.min())\n",
    "    # y = np.log(1 + np.log(y / (1 - y)))\n",
    "    return y\n",
    "\n",
    "\n",
    "# not used so far\n",
    "def preprocess_data(df):\n",
    "    df[CATEGORICAL_VARIABLES] = df[CATEGORICAL_VARIABLES].fillna(\"Unknown\")\n",
    "    df[OTHER_NUMERICAL_VARIABLES] = df[OTHER_NUMERICAL_VARIABLES].fillna(df[OTHER_NUMERICAL_VARIABLES].median())\n",
    "\n",
    "    return df\n",
    "\n",
    "# not used so far\n",
    "def features_engineering(df):\n",
    "    # Change year_hct to relative year from 2000\n",
    "    df['year_hct'] = df['year_hct'] - 2000\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(hparams=None):\n",
    "    FOLDS, oof_xgb, pred_xgb, train = make_predictions(hparams)\n",
    "\n",
    "    # COMPUTE AVERAGE TEST PREDS\n",
    "    pred_xgb /= FOLDS\n",
    "\n",
    "    y_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "    y_pred = train[[\"ID\"]].copy()\n",
    "    y_pred[\"prediction\"] = oof_xgb\n",
    "    m = score_f(train.copy(), y_pred.copy(), \"ID\")\n",
    "    plt.hist(oof_xgb[train.efs==1], bins=np.linspace(-3, 3, 200), alpha=0.5, label=\"Event\")\n",
    "    plt.hist(oof_xgb[train.efs==0], bins=np.linspace(-3, 3, 200), alpha=0.5, label=\"No event\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(f\"\\nOverall CV for XGBoost KaplanMeier =\", m)\n",
    "    return pred_xgb\n",
    "\n",
    "\n",
    "def make_predictions(hparams):\n",
    "    if hparams is None:\n",
    "        hparams = dict(\n",
    "            max_depth=6,\n",
    "            colsample_bytree=0.5,\n",
    "            subsample=0.8,\n",
    "            n_estimators=3000,\n",
    "            learning_rate=0.01,\n",
    "            min_child_weight=40,\n",
    "            gamma=1,\n",
    "            eta=0.0,\n",
    "            reg_lambda=0.1,\n",
    "            reg_alpha=0.1,\n",
    "            eps=2e-2,\n",
    "            eps_mul=1.01,\n",
    "            pos_shift=0.2\n",
    "        )\n",
    "    FEATURES, test, train, y = prepare_data(hparams.pop('eps'), hparams.pop('eps_mul'))\n",
    "    FOLDS = 5\n",
    "    kf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "    oof_xgb = np.zeros(len(train))\n",
    "    pred_xgb = np.zeros(len(test))\n",
    "    pos_shift = hparams.pop('pos_shift')\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(train, train.race_group)):\n",
    "        print(\"#\" * 25)\n",
    "        print(f\"### Fold {i + 1}\")\n",
    "        print(\"#\" * 25)\n",
    "        x_test, x_train, x_valid, y_train, y_valid = _prepare_training_data(\n",
    "            FEATURES, test.copy(), test_index, train.copy(), train_index, y.copy(), pos_shift=pos_shift\n",
    "        )\n",
    "        hparams.update(\n",
    "            dict(\n",
    "                objective='reg:pseudohubererror',\n",
    "                max_cat_to_onehot=10,\n",
    "                device=\"cuda\",\n",
    "                enable_categorical=True,\n",
    "                random_state=42,\n",
    "                monotone_constraints={\n",
    "                    # 'comorbidity_score': 1,\n",
    "                    # 'hla_match_c_high': -1,\n",
    "                    # 'hla_high_res_10': -1,\n",
    "                    'hla_high_res_6': -1,\n",
    "                    'hla_high_res_8': -1,\n",
    "                    # 'hla_low_res_10': -1,\n",
    "                    'hla_low_res_6': -1,\n",
    "                    # 'hla_low_res_8': -1,\n",
    "                    'hla_match_a_high': -1,\n",
    "                    # 'hla_match_a_low': -1,\n",
    "                    # 'hla_match_b_high': -1,\n",
    "                    'hla_match_drb1_low': -1,\n",
    "                    'hla_match_c_low': -1,\n",
    "                    # 'hla_match_c_high': -1,\n",
    "                    # 'donor_age': -1,\n",
    "                    # 'hla_match_drb1_high': -1,\n",
    "                    'hla_match_dqb1_low': -1,\n",
    "                    'hla_nmdp_6': -1,\n",
    "                    # 'karnofsky_score': -1,\n",
    "\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        print(hparams['n_estimators'])\n",
    "        model_xgb = XGBRegressor(\n",
    "            **hparams\n",
    "        )\n",
    "        tt = train.loc[train_index]\n",
    "\n",
    "        array = np.array([.95 if x else 1 for x in ((tt.efs_time < 24) & (tt.efs == 0)).values])\n",
    "        array /= np.array([1.3 if x else 1 for x in tt.efs_time > 36])\n",
    "        model_xgb.fit(\n",
    "            x_train, y_train,\n",
    "            eval_set=[(x_train, y_train), (x_valid, y_valid)],\n",
    "            verbose=100,\n",
    "            sample_weight=array\n",
    "        )\n",
    "\n",
    "        # INFER OOF\n",
    "        oof_xgb[test_index] = model_xgb.predict(x_valid)\n",
    "        # INFER TEST\n",
    "        pred_xgb += model_xgb.predict(x_test)\n",
    "    return FOLDS, oof_xgb, pred_xgb, train\n",
    "\n",
    "\n",
    "def _prepare_training_data(FEATURES, test, test_index, train, train_index, y, pos_shift=0.1):\n",
    "    y[train.efs == 0] = y[train.efs == 1].min() - pos_shift\n",
    "    std = np.std(y[train_index])\n",
    "    x_train = train.loc[train_index, FEATURES].copy()\n",
    "    y_train = y[train_index] / std\n",
    "    x_valid = train.loc[test_index, FEATURES].copy()\n",
    "    y_valid = y[test_index] / std\n",
    "    x_test = test[FEATURES].copy()\n",
    "    # ind = (train.loc[train_index].efs_time < 24) | (train.loc[train_index].efs == 0)\n",
    "    # x_train = x_train[ind]\n",
    "    # y_train = y_train[ind]\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    val = train.loc[test_index]\n",
    "    return x_test, x_train, x_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(eps=2e-2, eps_mul=1.1):\n",
    "    test = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\n",
    "    test = add_features(test)\n",
    "    train = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\n",
    "    train = add_features(train)\n",
    "    train[\"y\"] = transform_survival_probability(train, time_col='efs_time', event_col='efs')\n",
    "    RMV = [\"ID\", \"efs\", \"efs_time\", \"y\"]\n",
    "    FEATURES = [c for c in train.columns if not c in RMV]\n",
    "\n",
    "    CATS = []\n",
    "    for c in FEATURES:\n",
    "        if train[c].dtype == \"object\":\n",
    "            CATS.append(c)\n",
    "            train[c] = train[c].fillna(\"NAN\")\n",
    "            test[c] = test[c].fillna(\"NAN\")\n",
    "    print(f\"In these features, there are {len(CATS)} CATEGORICAL FEATURES: {CATS}\")\n",
    "    print(f\"In these features, there are {len(CATS)} NUMERICAL FEATURES: {sorted([f for f in FEATURES if f not in CATS])}\")\n",
    "    combined = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "    # print(\"Combined data shape:\", combined.shape )\n",
    "    # LABEL ENCODE CATEGORICAL FEATURES\n",
    "    for c in FEATURES:\n",
    "\n",
    "        # LABEL ENCODE CATEGORICAL AND CONVERT TO INT32 CATEGORY\n",
    "        if c in CATS:\n",
    "            combined[c], _ = combined[c].factorize()\n",
    "            combined[c] -= combined[c].min()\n",
    "            combined[c] = combined[c].astype(\"int32\")\n",
    "            combined[c] = combined[c].astype(\"category\")\n",
    "\n",
    "        # REDUCE PRECISION OF NUMERICAL TO 32BIT TO SAVE MEMORY\n",
    "        else:\n",
    "            if combined[c].dtype == \"float64\":\n",
    "                combined[c] = combined[c].astype(\"float32\")\n",
    "            if combined[c].dtype == \"int64\":\n",
    "                combined[c] = combined[c].astype(\"int32\")\n",
    "    train = combined.iloc[:len(train)].copy()\n",
    "    test = combined.iloc[len(train):].reset_index(drop=True).copy()\n",
    "    y = train.y.copy()\n",
    "    # y = np.log(y)\n",
    "    y = (y - y.min() + eps) / (y.max() - y.min() + eps_mul * eps)\n",
    "    y = np.log(y / (1 - y))\n",
    "    plt.hist(y[train.efs == 1], bins=np.linspace(-4, 8, 150), label=\"Event\")\n",
    "    plt.hist(y[train.efs == 0], bins=np.linspace(-4, 8, 150), label=\"No event\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return FEATURES, test, train, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb = run(None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
